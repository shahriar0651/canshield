{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PlaidML. Make sure you follow this \n",
    "import os\n",
    "try:\n",
    "    import plaidml.keras\n",
    "    plaidml.keras.install_backend()\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "    from keras import backend as K\n",
    "    # example of training a gan on mnist\n",
    "    import keras\n",
    "    from keras.optimizers import Adam\n",
    "    from keras import Sequential\n",
    "    from keras import layers\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Reshape\n",
    "    from keras.layers import Flatten\n",
    "    from keras.layers import Conv2D\n",
    "    from keras.layers import Conv2DTranspose, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D\n",
    "    from keras.layers import LeakyReLU\n",
    "    from keras.layers import Dropout\n",
    "    from keras.models import load_model\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    #------------------------------\n",
    "    print(\"Running on MAC OS\")\n",
    "except:\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.layers import Reshape\n",
    "    from tensorflow.keras.layers import Flatten\n",
    "    from tensorflow.keras.layers import Conv2D\n",
    "    from tensorflow.keras.layers import Conv2DTranspose, MaxPooling2D, UpSampling2D, ZeroPadding2D, Cropping2D\n",
    "    from tensorflow.keras.layers import LeakyReLU\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    print(\"Running on Server/PC\")\n",
    "    #------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "from os.path import exists as file_exists    \n",
    "\n",
    "# Libraries\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import joblib\n",
    "#------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "# import visualkeras\n",
    "\n",
    "# Metric\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "#------------------------------\n",
    "\n",
    "import scipy\n",
    "from scipy.cluster import hierarchy as sch\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "#--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- Create Folders -------------\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory created!\\n{directory}\")\n",
    "def find_r_from_z (data):\n",
    "    return np.tanh(data)\n",
    "\n",
    "def find_z_from_r (data):\n",
    "    return np.arctanh(data)\n",
    "\n",
    "def rename_sigids(dataframe):\n",
    "    new_list = {}\n",
    "    try:\n",
    "        list_of_sigs = list(dataframe.columns)\n",
    "        dtype = 'df'\n",
    "    except:\n",
    "        list_of_sigs = list(dataframe).copy()\n",
    "        dtype = 'list'\n",
    "        \n",
    "    for sig in list_of_sigs:\n",
    "        sigs = sig.split(\"_\")\n",
    "        if len(sigs) != 5:\n",
    "            new_list[sig] = sig\n",
    "        else:  \n",
    "            sig_no = sigs[1]\n",
    "            id_no = sigs[-1]\n",
    "            if dataset == 'syncan':\n",
    "                new_list[sig] = f\"S:{sig_no.zfill(1)}_ID:{id_no.zfill(2)}\"\n",
    "            else:\n",
    "                new_list[sig] = f\"S:{sig_no.zfill(1)}_ID:{id_no.zfill(4)}\"\n",
    "\n",
    "            \n",
    "    if dtype == 'list':\n",
    "        return list(new_list.values())\n",
    "    else:\n",
    "        dataframe = dataframe.rename(columns=new_list)\n",
    "        dataframe = dataframe.rename(index=new_list)\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Creating two lists of ambient and attack datasets\n",
    "cur_dir= os.getcwd()\n",
    "dataset = 'syncan'\n",
    "dataset = 'road'\n",
    "\n",
    "\n",
    "ambient_dirs = glob.glob(cur_dir+f\"//..//data//{dataset}//generated//ambients//*.csv\")\n",
    "attack_dirs = glob.glob(cur_dir+f\"//..//data//{dataset}//generated//attacks//*.csv\")\n",
    "\n",
    "# Creating two lists of file names \n",
    "ambient_files = [x.split(\"/\")[-1].split(\".\")[0][0:-10] for x in ambient_dirs]\n",
    "attack_files = [x.split(\"/\")[-1].split(\".\")[0][0:-10] for x in attack_dirs]\n",
    "\n",
    "print(f\"Ambient files are:\\n {ambient_files[0:]}, etc.\")\n",
    "print(f\"\\n\\nAttack files are:\\n {attack_files[0:]}, etc.\")\n",
    "\n",
    "# %%\n",
    "# ambient_files\n",
    "\n",
    "# %%\n",
    "# Defining parameters..........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambient_files =ambient_files[2:]\n",
    "ambient_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del corr_df_mean, corr_df_max, z_fisher\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# reading each dataset in a df file\n",
    "folder_name_data = \"Saved_Data\"\n",
    "folder_name_plots = \"overall\"\n",
    "\n",
    "ensure_dir(f\"{folder_name_data}//\")\n",
    "ensure_dir(f\"plots//{folder_name_plots}//\")\n",
    "\n",
    "if dataset == 'syncan':\n",
    "    fig, axes = plt.subplots(2, 2, figsize = (10,6))\n",
    "else:\n",
    "    fig, axes = plt.subplots(3, 4, figsize = (16,8))\n",
    "    \n",
    "axs = axes.flatten()\n",
    "df_scale = pd.DataFrame([])\n",
    "\n",
    "for indx, (file_name, file_dir) in enumerate(zip(ambient_files, ambient_dirs)):\n",
    "    ax = axs[indx]\n",
    "#     if file_name !=  'ambient_dyno_drive_basic_short':\n",
    "#         continue\n",
    "        \n",
    "    print(file_name)\n",
    "\n",
    "    try:\n",
    "        #Try loading corr mat.............\n",
    "#         regenerate\n",
    "        corr_df_new = abs(pd.read_csv(f\"{folder_name_data}//Corr_matrix_update_{file_name}.csv\" ,index_col=0).fillna(0))\n",
    "        print(f\"Loaded {file_name}\")\n",
    "        \n",
    "    except:\n",
    "        #Generate corr mat and store......\n",
    "        print(\"Loading dataset: \",file_name)\n",
    "        # Checking if the signalwise data already exists\n",
    "        X_train = pd.read_csv(file_dir, index_col = 0) \n",
    "\n",
    "        # Defining the number of signals..............................\n",
    "        X_train = X_train.copy()\n",
    "        X_train = X_train.drop(columns = ['Label', 'Time', 'ID']).copy()\n",
    "\n",
    "        if dataset == 'road':\n",
    "            # Forward filling algorithm........\n",
    "            print(\"Forward filling...\")\n",
    "            X_train = X_train.ffill().copy()\n",
    "            X_train = X_train.bfill().dropna()   \n",
    "            #--------------------------------\n",
    "            print(\"X_train.shape\", X_train.shape)\n",
    "            #------------------------------------\n",
    "        else:\n",
    "            print(\"No treatment needed for SynCAN!\")\n",
    "        \n",
    "        # Loading data..........................\n",
    "        # X_train = X_train.values\n",
    "        \n",
    "        #Saving scaler data.........................................\n",
    "        scaler_train = MinMaxScaler()\n",
    "        scaler_train.fit(X_train.values)\n",
    "        df_scale[f'{file_name}_max'] = list(scaler_train.data_max_)\n",
    "        df_scale[f'{file_name}_min'] = list(scaler_train.data_min_)\n",
    "        #............................................................\n",
    "\n",
    "        #Correlation matrix..........................................\n",
    "        corr_df_new = X_train[0::100].corr().fillna(0).copy()\n",
    "        #Saving corr mat.......................\n",
    "        corr_df_new.to_csv(f\"{folder_name_data}//Corr_matrix_update_{file_name}.csv\", index = True, header = True)\n",
    "        #Corr mat is loaded...........................................\n",
    "\n",
    "    try:\n",
    "#         print(\" Updating pixels..... 1\")\n",
    "        corr_df_mean += corr_df_new\n",
    "        col_list = corr_df_new.columns.to_list()\n",
    "        indeces = corr_df_new.loc[col_list,col_list] > corr_df_max.loc[col_list,col_list] \n",
    "        corr_df_max[indeces] = corr_df_new[indeces].copy()\n",
    "#         corr_df_max[corr_df_new > corr_df_max] = corr_df_new[corr_df_new > corr_df_max].copy()\n",
    "        z_fisher += find_z_from_r(corr_df_new.values)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Failed to upload to ftp: '+ str(e))\n",
    "        print(\"*********** Adding new Corr *************\")\n",
    "        columns_list = corr_df_new.columns.tolist()\n",
    "        corr_df_mean = corr_df_new.copy()\n",
    "        corr_df_max = corr_df_new.copy()\n",
    "        z_fisher = find_z_from_r(corr_df_new.values)\n",
    "        \n",
    "    ax = sns.heatmap(corr_df_new, ax = ax, xticklabels=False, yticklabels=False)\n",
    "    ax.set_title(file_name)\n",
    "#     ax.tick_params(left=False, bottom=False) ## other options are right and top\n",
    "fig.suptitle(\"Correlation heatmaps of different files\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots//{folder_name_plots}//Corr_Heatmaps_{dataset}.jpg\", dpi = 350) \n",
    "plt.show()\n",
    "\n",
    "# Finalizing the corr mats.......................\n",
    "corr_df_mean = (corr_df_mean/len(ambient_files)).fillna(0)\n",
    "z_fisher = z_fisher/len(ambient_files)\n",
    "corr_df_fisher = pd.DataFrame(find_r_from_z(z_fisher)).fillna(0)\n",
    "corr_df_fisher.columns = corr_df_mean.columns\n",
    "corr_df_fisher.index = corr_df_mean.index\n",
    "\n",
    "try:\n",
    "    df_scale.index = X_train.columns\n",
    "    df_scale = df_scale.T.copy()\n",
    "    df_scale.to_csv(f\"Scalling_Data/min_max_values_{dataset}.csv\", header=True, index=True)\n",
    "    #.................................................\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(4,5, figsize = (12, 4.5), sharex = True)\n",
    "# for sig, ax in zip(top_signals, axes.flatten()):\n",
    "#     X_train[sig][0:1000000].plot(ax = ax, marker = 'p', linestyle = '--', markersize= '0.5')\n",
    "#     ax.set_title(sig)\n",
    "#     ax.set_xlabel(\"Time Step\")\n",
    "# fig.suptitle(\"Time series plot of different signals\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"plots//{folder_name_plots}//Time_series_{dataset}.jpg\", dpi = 350) \n",
    "# plt.savefig(f\"plots//{folder_name_plots}//Time_series_{dataset}.pdf\") \n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(corr_df_mean, vmin = 0, vmax = 1 )\n",
    "# fig.suptitle(\"Correlation heatmaps of different files\")\n",
    "plt.title(\"Overall correltion heatmap: Mean\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots//{folder_name_plots}//Corr_Heatmap_Overall_{dataset}_Mean.jpg\", dpi = 350) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(corr_df_fisher, vmin = 0, vmax = 1 )\n",
    "# fig.suptitle(\"Correlation heatmaps of different files\")\n",
    "plt.title(\"Overall correltion heatmap: Z Fischer\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots//{folder_name_plots}//Corr_Heatmap_Overall_{dataset}_Fischer.jpg\", dpi = 350) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_Signals = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'syncan':\n",
    "    top_signals = list(corr_df_fisher.columns)\n",
    "else:\n",
    "    # Opening attacked signals file\n",
    "    #-----------------------\n",
    "    f = open(cur_dir+f\"//Saved_Data//attacked_signals.json\")\n",
    "    attacked_signals = json.load(f)\n",
    "    f.close()\n",
    "    x = [] \n",
    "    for val in attacked_signals.values(): x +=val\n",
    "    targeted_signals = list(set(x))\n",
    "\n",
    "    # Top 20 signals........................\n",
    "    top_signals_df = pd.DataFrame([])\n",
    "\n",
    "    for targeted_signal in targeted_signals:\n",
    "        top_signals_df[targeted_signal] = corr_df_fisher[targeted_signal].sort_values(ascending=False).index\n",
    "\n",
    "    # ......................\n",
    "    top_signals = targeted_signals.copy()\n",
    "    row = 0\n",
    "    col = 0\n",
    "\n",
    "    while(len(top_signals) < num_of_Signals):\n",
    "\n",
    "        sig = top_signals_df.iloc[row, col]\n",
    "        #print(f\"{row}, {col} : {sig}\")\n",
    "        if sig not in top_signals:\n",
    "            top_signals.append(sig)\n",
    "    #         print(f\"Adding {sig}\")\n",
    "        col += 1  \n",
    "\n",
    "        if col == len(targeted_signals):\n",
    "            #print(\"going to the next row...\")\n",
    "            col = 0\n",
    "            row += 1      \n",
    "    #     print(len(top_signals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "from scipy.cluster import hierarchy as sch\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize = (17, 5))\n",
    "\n",
    "#------------ Clustering again ---------------\n",
    "# Plotting the correlation matrix...........\n",
    "\n",
    "# corr_df_target = corr_df_fisher.copy()\n",
    "\n",
    "corr_df_target = corr_df_mean.copy()\n",
    "\n",
    "\n",
    "top_signals = rename_sigids(top_signals)\n",
    "corr_df_target = rename_sigids(corr_df_target)\n",
    "\n",
    "corr_df = abs(corr_df_target.loc[top_signals,top_signals]).copy()\n",
    "\n",
    "\n",
    "sns.heatmap(corr_df, ax = axes[0]) # unclustered version\n",
    "# plt.savefig(f\"plots/heatmap_before_{file_name}.jpg\", dpi = 500)\n",
    "# plt.show()\n",
    "axes[0].set_title(\"Correlation before clustering\")\n",
    "\n",
    "#--------------------------------------------\n",
    "corr_array = corr_df.copy()\n",
    "inplace = False\n",
    "pairwise_distances = sch.distance.pdist(corr_array)\n",
    "linkage = sch.linkage(pairwise_distances, method='complete')\n",
    "\n",
    "#--------------------------------------------\n",
    "# plt.figure(figsize=(5, 5)) \n",
    "axes[1].set_title(\"Dendrograms of hierarchical clustering\")\n",
    "dend = sch.dendrogram(linkage, orientation='right', labels=corr_df.index, ax =axes[1])\n",
    "#--------------------------------------------\n",
    "# Plotting the correlation matrix...........\n",
    "sns.heatmap(corr_df.loc[dend['ivl'][::-1], dend['ivl'][::-1]], ax = axes[2])\n",
    "axes[2].set_title(\"Correlation after clustering\")\n",
    "\n",
    "if dataset == 'syncan':\n",
    "    fig.suptitle(f\"Signal clustering and rearranging based on correlations: SynCAN dataset\")\n",
    "else:\n",
    "    fig.suptitle(f\"Signal clustering and rearranging based on correlations: ROAD dataset\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"plots//{folder_name_plots}//heatmap_before_after_den_{dataset}.jpg\", dpi = 500)\n",
    "plt.show()\n",
    "\n",
    "#Updating the columns sequence\n",
    "#--------------------------------------------\n",
    "# print(dend['ivl'][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 50\n",
    "\n",
    "# list_of_sigs = dend['ivl'][::-1]\n",
    "# list_of_sigs\n",
    "# ['S:4_ID:0167',\n",
    "#  'S:2_ID:1505',\n",
    "#  'S:4_ID:1031',\n",
    "#  'S:1_ID:0051',\n",
    "#  'S:4_ID:0778',\n",
    "#  'S:8_ID:0778',\n",
    "#  'S:4_ID:1628',\n",
    "#  'S:2_ID:1255',\n",
    "#  'S:2_ID:0526',\n",
    "#  'S:1_ID:1760',\n",
    "#  'S:4_ID:1176',\n",
    "#  'S:6_ID:0208',\n",
    "#  'S:3_ID:1760',\n",
    "#  'S:4_ID:1760',\n",
    "#  'S:2_ID:1760',\n",
    "#  'S:1_ID:0167',\n",
    "#  'S:7_ID:0208',\n",
    "#  'S:5_ID:1590',\n",
    "#  'S:7_ID:1628',\n",
    "#  'S:9_ID:1413',\n",
    "#  'S:9_ID:0167',\n",
    "#  'S:8_ID:1455',\n",
    "#  'S:5_ID:0640',\n",
    "#  'S:2_ID:0852',\n",
    "#  'S:3_ID:0622',\n",
    "#  'S:2_ID:1398',\n",
    "#  'S:9_ID:1455',\n",
    "#  'S:3_ID:1076',\n",
    "#  'S:11_ID:1031',\n",
    "#  'S:10_ID:0470',\n",
    "#  'S:1_ID:0683',\n",
    "#  'S:2_ID:0778',\n",
    "#  'S:6_ID:0628',\n",
    "#  'S:6_ID:0167',\n",
    "#  'S:3_ID:0208',\n",
    "#  'S:1_ID:0852',\n",
    "#  'S:7_ID:1788',\n",
    "#  'S:2_ID:0051',\n",
    "#  'S:7_ID:0354',\n",
    "#  'S:3_ID:0996',\n",
    "#  'S:1_ID:1634',\n",
    "#  'S:2_ID:0208',\n",
    "#  'S:1_ID:0014',\n",
    "#  'S:3_ID:0014',\n",
    "#  'S:8_ID:0470',\n",
    "#  'S:2_ID:0061',\n",
    "#  'S:2_ID:0204',\n",
    "#  'S:3_ID:0458',\n",
    "#  'S:5_ID:1372',\n",
    "#  'S:5_ID:0631']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10\n",
    "\n",
    "# list_of_sigs = dend['ivl'][::-1]\n",
    "# list_of_sigs\n",
    "# ['S:4_ID:1628',\n",
    "#  'S:2_ID:1255',\n",
    "#  'S:3_ID:1760',\n",
    "#  'S:6_ID:0208',\n",
    "#  'S:1_ID:1760',\n",
    "#  'S:4_ID:1760',\n",
    "#  'S:2_ID:1760',\n",
    "#  'S:5_ID:1590',\n",
    "#  'S:3_ID:0208',\n",
    "#  'S:3_ID:0014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Top 5\n",
    "# # list_of_sigs = dend['ivl'][::-1]\n",
    "# # list_of_sigs\n",
    "# ['S:3_ID:1760',\n",
    "#  'S:6_ID:0208',\n",
    "#  'S:1_ID:1760',\n",
    "#  'S:4_ID:1760',\n",
    "#  'S:2_ID:1760',\n",
    "#  'S:2_ID:1255',\n",
    "#  'S:3_ID:0208']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
